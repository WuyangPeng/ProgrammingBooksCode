Study 1-Study 20
Copyright  (c) 1997 A. Watt and L. Cooper.
Permission to copy without fee any or all of this material is granted, provided that the copies are not made or distributed for direct commercial advantage, the copyright notice and title of the image(s) appear and notice is given that copying is by permission of the copyright holder. To copy otherwise, or to republish, requires specific permission.

Study 21 (source images)
Copyright (c) 1990-1996, 1997 LightWork Design Limited. All rights reserved.

Learning Rendering

This resource is series of images/animations demonstrating the options available in mainstream rendering programs. The series is intended to support an undergraduate or post graduate course in rendering techniques, an instructor using the resource as appropriate to his/her particular course. 

Most images that illustrate a rendering method are highly tuned and are consequently 
of little educational value. The idea of the series is to demonstrate both the potential of the techniques and their deficiencies and disadvantages. Apparent in the images are the effects of the many and varied shortcomings in the methods that tend to be overlooked in standard texts. Another problem in studying images produced by renderers is that many subtle differences are lost in colour reproductions.

One of the motivations of the series is to enable a study of renders and their behaviour without having to set them going and wait the (usually) long time for images to complete. It is hoped that the large number of images facilitates this.

The main series is structured around an old idea - to use the same scene as far as possible and inject it into number of renderers and to use different options within a rendering method.

The image descriptions, although reasonably comprehensive, are not meant to explain how each method works. Instead they  describe the important visual  ramifications of the algorithms. In other words they are meant to be used  in conjunction with normal textbook type explanations and therefore assume that the viewer has already some familiarity with the basic operation of the algorithms. 

Ideally the imagery should be viewed directly on a monitor screen, but any image could be used to create 35mm slides for lecture presentation. Note that the zoom sequences refer to an image regeneration of an appropriate part of the full sized image at a higher resolution. In other words they are generated by moving the viewpoint  towards a point of interest in the scene - they are not image zooms.

The set consists of several hundred images which are all 24bit stored in a compressed (lossless) tif format which should be compatible with most popular image browsers. It is recommended that a browser with a zoom facility is used so that areas of interest can be studied more closely. The set also contains several animations. These are stored in the standard PC AVI format.

The series consists of the following general topics:

* Basic Camera Options

* Basic Rendering Techniques for Polygon Mesh Scenes

* Material Realism (alternative reflection models)

* Scanline Rendering Techniques (texture mapping, bump mapping, environment mapping).

* Simple Light Source Options

* Anti-Aliasing

* Global Illumination 

* Ray Tracing

* Radiosity

* Colour and Rendering

* Non-Photo Realistic Rendering

The image descriptions that follow refer to single image, series of images or animations. The descriptions are organised into 'studies'.



















STUDY 1	

BENCHMARK SCENE WIREFRAME AND 'VIRTUAL' CAMERA PARAMETERS

wrfrm  wireframe of the main scene used throughout this set. Note the polygonal modelling resolution. Objects with large planar surfaces are not subdivided. This causes problems in many rendering methods.
hdline	hidden line removal.
hdlabv	hidden line from above (parallel projection)

CLIP PLANES

clpln1	front and back clip planes
clpln2	using front clip plane to see inside geometry

Manipulating the position of clip planes in a viewing frustum is of limited practical utility, but demonstrates one of the main ways in which a computer graphics 'camera' differs from a real camera. Clip plane manipulation has applications in interactive examination of data sets in volume rendering. 

EXTERNAL CAMERA PARAMETERS

The next series of images shows the variation of external camera parameters. A usefully compact specification for perspective projection is field of view (fov). For a particular field of view, the distance of the image plane from the centre of projection is adjusted so that the projection onto the image plane is always the same size

fov1  to fov8	 fov varying from 20 to 160 degrees.

orth	
An orthographic projection is specified by a viewplane windows with respect to world distances. The difference between an orthographic and perspective projection is best seen by comparing this image with the nearest perspective view  (fov4).













STUDY 2  BASIC RENDERING OPTIONS FOR POLYGON MESH OBJECTS

ambpng	
Phong shaded  scene - ambient term only.
In any Phong shaded scene the ambient lighting intensity, which compensates for the lack of any global illumination calculations,  is set to a fairly low level. It has to illuminate areas of the scene that cannot 'see' a light source and at the same time not dominate the shaded parts of the scene.

adpng 		
Phong shaded - ambient and diffuse terms.
The illumination of wall from wall lights appears to be in
 the wrong place. This is due to the screen space interpolation which operates with equal distances (pixels) and ignores the fact that a perspective transformation has taken place mapping equal distances along the wall into varying distances.  This is one of the classic problems with Phong shading.

Another problem, often ignored, is what to do with visible light sources. Treating them as a shaded object doesn't quite work. In this case the point light source is inside the surrounding sphere and cannot by definition illuminate it. We can only get illumination from the wall light and turn up the ambient component for the light object The wall light are spotlights with the cone angle coinciding with the geometry of the light source.

adspng 	Phong shaded - ambient  + diffuse  + specular.

kdksXX 	
Phong shaded - array of images showing a Phong shaded teapot with 
different Ks and Kd  ranging from 0.0 to 1.0 in 0.2 steps (6x6=36 images). The 'traditional' way of illustrating the shading equation parameters. (Ka=0.7, exponent = 10.0 in all images)

kdksfl	montage of the kdksXX image array.

ksxpXX	
Phong shaded - 5x5 array of images showing different Ks and exponent 
values (with Ka=0.7, Kd=1.0). Ks is increasing across and exponent is 
increasing down. Values used are Ks: 0.0, 0.25, 0.5, 0.75, 1.0.
exponent: 10.0, 30.0, 90.0, 270.0, 810.0
Note Phong illumination model attempts to model increasing 
"shininess" by simply making size of specular highlight smaller. 

ksxpfl	
Montage of the ksxpXX images. Note that although increasing the specular exponent is supposed to make the surface more shiny, all it does is to make it appear that the light source has shrunk in extent. This is because, in reality, specular highlights are just reflections of a light source with extent; we prefer this interpretation of the images, rather than the unlikely event that we are seeing a point light source interacting with a surface of different shininess.

cmpflt	
Full scene flat shaded. flat shading shows the polygonal nature of the surfaces due to discontinuities in intensity.  Shading changes across large polygons do not appear (e.g. illumination of the wall lights on the wall).

cmpgr		
Full scene Gouraud shaded. The illumination on the walls due to the spot lights does not appear. Note artefact on back wall (see image cpgrs and cpgrsw below). The polygon edges are still slightly visible due to discontinuities in intensity gradient - the characteristic Mach  banding effect. This effect persists in radiosity imagery where a form of Gouraud interpolation is used to interpolate a constant radiosity (per patch) solution. There is a parallel between the flat/Gouraud image and basic radiosity solution/final solution (see later images).

(adspng)	see the image (above) for equivalent Phong shaded image.

cmpftz 	Flat shaded - zoom in on one of the wall lights.
cmpgrz	ditto -Gouraud shaded 
cmppgz	
ditto -Phong shaded Note that if polygon mesh resolution of objects is fairly high then the difference between Gouraud and Phong can be fairly subtle -  as the size of the polygons approaches single pixels in screen space then the shading will approach Phong shading.

cmpflz 	 Flat shaded - resolution of the light body  has now  been reduced. 

cmpglz	Gouraud shaded	
		Phong highlight is lost since none of the mesh vertices are close to it.
cmpplz	Phong shaded.
		Phong highlight reappears.

cmpdf1	
difference image between cmpgr (Gouraud scene) and adspng (Phong scene). Note that most differences appear on the large walls.  There is little difference around the Phong highlights since mesh resolutions are high. (Both images were converted to grey scale, then the difference image was generated  and  histogram stretched.

cmpdf2	difference between cmpglz and cmpplz - big difference due to low resolution mesh.

cpgrsw      
When Gouraud shading large polygons, artefacts can appear see back wall
in (cmpgr). This is due to the scan line conversion algorithm switching between different edges as it scans down the polygon. Here we remove this by splitting the polygon into triangles.

cpgrs		Gouraud shaded scene with triangulated polygon.

STUDY 3  'MATERIAL REALISM'

One of the early enhancements to basic local reflection models was to try to make materials less plastic like and more like, for example, metal if that is what is required. Phong parameters can only control colour of the material (and also colour of the specular highlight). The look of a polished material depends on such subtleties like the way in which the colour and intensity of a reflected highlight change as a function of the incident angle of the light and the material. Metals like gold, for example have distinct yellow highlights for incident white light. This is modelled by using  Fresnel's law for the behaviour of light at a polished surface.

mtcd_f	 montage of objects shaded to simulate various materials as follows

Image name
Material type
mtcd1
iron
mtcd2
steel
mtcd3
stainless steel
mtcd4
machine steel
mtcd5
antique brass
mtcd6
polished brass
mtcd7
copper
mtcd8
bronze
mtcd9
nickel
mtcd10
zinc
mtcd11
lead
mtcd12
cast aluminium
mtcd13
machined aluminium
mtcd14
magnesium
mtcd15
gold
mtcd16
burnished gold
mtcd17
polished gold
mtcd18
silver
mtcd19
silver plate
mtcd20
tungsten
mtcd21
platinum
mtcd22
chromium
mtcd23
chromium plate
mtcd24
graphite
mtcd25
mercury


For example, compare images mtcd6 (polished brass) and mtcd17 (polished gold) the precise differences between these would be difficult to obtain by fine tuning Phong parameters.

The images are in fact ray traced to further add to the realism, but the model can be used exclusively as a local reflection model. One of the difficulties here is that the shading equations are only evaluated at 3 wavelengths and 'colour aliasing' occurs disrupting the carefully tuned material parameters (see Colour Study below)

mtcd1-25 	 Individual images from the above montage.

phng
cdtr
diel
are three animations showing a light source rotating in a circle around, in each case, the same object. The object has been rendered using Phong and a metallic shader in the first and second animation. In the third case the object is rendered as glass using a ray tracer.






































STUDY 4	TEXTURE MAPPING

2D AND 3D TEXTURE MAPPING

txtm		
	Scene with texture maps on floor, doors and desk. 

txsld1	Solid wood texture map

txsld2	
Solid marble texture map. These are examples of three-dimensional texture fields that are procedurally defined. The problems of 2D to 3D mapping is completely avoided with this technique where the mapping is one to one. Note that texture in rendering usually means, as it does here, modulating  the diffuse and ambient coefficients in the shading equation.

txiaXX	Array of solid marble textures with different parameters		
		5x5 array = 25 images.
		Across is increasing "grain scale" determines size of 
		grain crystals.
		Down is increasing "detail" - this increases the complexity of the 
		pattern.

txia_m 	montage of above image array.

txim1		planar texture map of image
txim2		cylindrical map 
txim3		spherical map
		
These are examples of 'two-part' texture mapping. The difficulties of mapping a two-dimensional image onto an arbitrarily shaped three-dimensional surface are overcome by using an intermediate surface. Effectively the texture map is 'pasted' onto the (easy) intermediate surface, and then mapped from this surface onto the object. The intermediate surfaces are a plane, a cylinder and a sphere.


BUMP MAPPING

bmpprc	Procedurally bump mapped object.
		
bmpim1	bump mapped based on a simple image (data/bpim1) cylindrical 
		texture space used
bmpim2	
Bump mapping is often combined with texture mapping. Here a bump map has been used to (apparently) perturb the surface and a coincident texture map to colour the 'bump objects'. (maps are data/bpim2 data/txim2)

bmpim3	bump mapping and texture mapping used on text 
		(maps are data/bpim3 data/txim3).

The classic texture mapping technique of bump mapping appears to modulate the surface geometry. Bump maps, which are just height fields, can be created procedurally or interactively - just like a normal 2D texture map. Examination of any image will confirm the main disadvantage of this technique - the geometry is not perturbed and thus the silhouette edge remains smooth.

ENVIRONMENT MAPPING

envm1		
Environment mapped teapot - low env. map resolution (32x32 for each of the six images.) The environment map is computed with the teapot removed from the scene and the view position at the centre of the teapot.

envm1z	
Zoom in on the teapot. The reflections appear pixelated due to low map resolution. Note that this is entirely tolerable for the normal sized image and can also be used to simulate a non-perfect specular reflector

envm1a	images making up the environment map (32x32 pixels)
envm1b	"
envm1c	"
envm1d	"
envm1e	"
envm1f	"

envm1m	montage of the above six images in an environment cube.

envm2		resolution of environment map images is  now increased to 128x128

envm2z
Zoom the in on teapot. Note that the  reflections are much sharper than before. The environment map is geometrically incorrect (see later comparison with a ray traced version). The 'incorrectness' is a function of the real size of the object in the environment. Another defect of environment mapping is that you cannot see self reflections (a  reflection of teapot itself). When the environment amp is generated the teapot is removed from the scene.

envm2a	images making up env map
envm2b	"
envm2c	"
envm2d	"
envm2e	"
envm2f	"
envm2m	montage of env images


envm3		
An attempt to use environment mapping to create reflections in the 
mirror on the wall. Here the problem is where to position the viewpoint for the 
environment map. With the resolution still at 128x128, the centre of the environment map is positioned just in front of the mirror surface. The reflections appear fuzzy even at the higher resolution because of the large planar surface.

envm3z	
Zoom in on  the mirror. The128x128 resolution works fine for teapot but not for the flat mirror because the teapot has curved surfaces and hence indexes into many parts of the environment map. However, with the mirror, the range of reflected angles is small and  only a small part of the environment map is used - hence the pixelated appearance of the reflections. This could be improved by increasing the resolution of the env map but the majority of it will still remain unused, wasting memory and computation. (see later for alternative method).

envm4		
This is a ray traced image of the teapot which uses geometrically correct reflections. Compare this to envm2z to show that environment mapping is not geometrically correct. The teapot now shows multiple reflections with itself.

envm5		
Here the size of the teapot has been increased by 50% to show that the geometric errors get worse as the size of the object increases - compare this with the next image.

envm6		ray traced image with bigger teapot.

envm7		
This image is to show that extra effects can be created by performing  image processing on the images which comprise the environment map.Here, a low pass filter has been applied which blurs the images. The effect is similar to a reflective surface which also causes slight scattering of the light.

envm7m	the environment cube for the above image.

envm8		
This to show that environment mapping can be used with real images as well as computer generated scenes.

envm8m	
The environment cube for the above image. Here the environment map was created 'artificially' from a 4 image panorama - the 4 centre images. The sky and ground images were created by arbitrarily/wrongly cutting and pasting from the panorama.

	
AN ALTERNATIVE METHOD FOR MODELLING REFLECTIONS (FLAT SURFACES)

The next four images illustrate an alternative method of modelling reflections in flat mirror surfaces (without resorting to ray tracing). The method consists of rendering the scene from a virtual camera position on the opposite side of the mirrored surface and then using part of this images for the reflection. The method is described in detail
below.

Figure 1. Alternative reflection method.

The first stage in the process involves producing a mask which defines those pixels in the required image which are part of the reflection in the planar mirrored surface. In this case, this was generated by setting the ambient, diffuse and specular parameters to zero for all the objects in the scene except the mirror, and then rendering the scene using Phong shading. This creates the mask as seen in  envmsk (note teapot silhouette).

The next stage involves rendering the scene using a new set of "virtual" camera "from" and "to" positions (the "from" and "to" parameters are simply one method of defining the position and orientation of the camera). The position of these virtual points is found by reflecting the real camera "from" and "to" positions in the plane which contains the mirrored surface, as seen in figure 1. This stage generates an image, part of which is the reflection which we require (see envmim ). 
The mask (envmsk) and the reflection image (envmim) are then flipped (left to right) and the mask is used to isolate the required part of this image (see envrfl).

An image of the scene from the real camera position is then rendered, and the mask is used to make the mirror area black.

Finally, the above image is added to envrfl to create the final image envfin.

Note :
1. The mirror reflections are geometrically correct.
2. Only a limited amount of inter-reflection can be produced. For example, we can see the reflection of the teapot in the mirror, but no deeper than that.


STUDY 5	SHADOW MAPPING OPTIONS

shmap	 
Adding shadows to the previous environment mapped scene using shadow mapping or shadow Z-buffer technique. Note that there is 'interaction' between shadow mapping and environment mapping; the order of the operations is important. In this image the environment map was computed before the shadow map and hence the shadow of the teapot does not appear in the reflections in the teapot.  Note also the shadow edge definition. The inadequacies here are due to the spatial resolution of the shadow map which is here 256x256 for each component. 

shmap2	
In this image the whole scene was generated including the teapot, the shadow map was then computed. The teapot was then removed from the scene and the environment map was computed (even though the teapot is removed its shadow still appears) and then the teapot was added and the scene rendered from the proper viewpoint. The  shadow of the teapot can now be seen in the reflections in the teapot - however they appear slightly wrong due to the errors in the environment mapping.

shmap3	
shadow map res = 128, softness = 0. Lowering the shadow map resolution increases the blocky look at the shadow edges and also causes shadows to break up - shadow of the leaves, for example (compare this image to shmap2).

shmap4	
increasing the shadow map resolution diminishes the blockiness. 

shdt1		depth map for scene from proper viewpoint

shdt2a		The next six images are the depth maps for the point light source (used 
		for explaining the shadow map technique).	
shdt2b 	
shdt2c
shdt2d
shdt2e
shdt2f

shdt2m	montage of the above six images (depth cube).


STUDY 6  SIMPLE LIGHT SOURCE OPTIONS

The following series demonstrates the common variations that can be made with point light sources and a local refection model such as Phong. Spotlights are basically point sources with the addition of a cone of influence. Objects can only interact with the light if they are within the cone.

liamb		ambient light source 

lidst		
Distant light source (parallel rays) eye light source (always positioned coincidentally with the camera)- there never any shadows from this.

lipnt		
Point light source positioned quite close to vase object - direction from point to centre of vase is same as "distant" light source direction. Note that the shadows are different to those created with distant light source - the incoming light rays are no longer parallel.

lieye
	Creates a point light source coincident with the eye; thus no shadows are 	produced.

lispt1	
Spot light in same position as point light - cone angle is 30 degrees. Note that there are apparently two shadows. The vase shadow is created normally. The spotlight shadow is not explicitly a shadow - just an area outside the cone of influence.

lispt2 
Spotlight cones can easily be modulated by defining a drop of zone which softens the edges of the 'shadow'. This image has a 'drop of' zone of 5 degrees

lispt3	
same as above but a  "beam distribution" parameter has been turned up. This makes the light within the main part of the beam drop off  away from the main axis.

faoff1	
the next  three images illustrate attenuated light sources - here the point light source is normal i.e. there is no drop off of the light as we move away from the light source.	

faoff2	here we have 1/d drop off.

faoff3	here we have 1/d2 drop off.

fgdc		
depth cueing - intensity of "white fog" increases linearly with depth. Starting at the nearest object in the scene and ending at the furthest. A much loved 'feature' of dubious worth.














































STUDY  7	ANTI-ALIASING

anta1	no anti-aliasing

anta2		
The normal/easy approach to anti-aliasing is to 'super sample' or render an image at a higher than screen resolution then filter the result 'down' to normal pixel size. This has been done in this image by generating a virtual image at 2x the screen resolution

anta3		
Same as previous but the virtual image is now 3x. Note that the extra work involved in this kind of brute force anti-aliasing is equal to the resolution factor squared. In this case going from 2X to 3X involves more than twice the amount of work but the difference in final image quality is negligible

anta4		
Ray tracing allows  context dependent anti-aliasing - instead of considering the whole image, we only anti-aliasing those areas of the initially generated image that require further consideration. This image uses adaptive anti-aliasing where extra 'sub-pixel' rays are generated as a function of the local image gradient.





























STUDY 8  GLOBAL ILLUMINATION

room_a   
	demonstration image rendered using a (complete) global illumination renderer 	(RADIANCE - see http://radsite.lbl.gov/radiance/HOME.html).

room_b
room_c
room_d   	 components of room_a

glob1		 main diagram showing a selection of global illumination paths

glob2 		a path handled by Whitted ray tracing

glob3		 a path handled by distributed ray tracing

glob4 		 a path handled by two-pass ray tracing

The above images use Heckbert's string notation for listing all the interactions that occur along a path of a light ray as it travels from source (L) to the eye (E). Here a light path from the light source to the first hit is termed L, subsequent paths involving transfer mechanisms at a surface point are categorised as DD, SD, DS or SS. glob1  shows an example of a simple scene and various paths. The path that finally terminates in the eye is called E. The paths in the example are:

1) LDDE   For this path the viewer sees the shadow cast by the table. The light reflects diffusely  from the right-hand wall on to the floor. Note that any light reflected from a shadow area must have a minimum of two interactions between L and E.

2) LDSE + LDDE  Here the user sees the dark side of the sphere which is not receiving any direct light. The light is modelled as a point source, so any area below the 'equator' of the sphere will be in shadow. The diffuse illumination reflected diffusely from the wall is directed towards the eye and because the sphere is shiny the refection to the eye is both specular and diffuse.

3) LSSE +LDSE  Light is reflected from the perfect mirror surface to the eye and the viewer sees a reflection of the opaque or coloured ball in the mirror surface.

4) LSDE Here the viewer sees a shadow area that is lighter than the main table shadow. This is due to the extra light reflected from the mirror and directed underneath the table.

5) LSSDE  this path has three interactions between L and E and the user sees a caustic on the table top which is a diffuse surface. The first specular interaction takes place at the top surface of the sphere and light from the point source is refracted through the sphere. There is a second specular interaction when the light emerges from the sphere and hits the diffuse table surface. The effect of the refection is to concentrate light rays travelling through the sphere into a smaller area on the table top than they would occupy if the transparent sphere was not present. Thus the user sees a bright area on the diffuse surface.

A complete global illumination algorithm would have to include any light path which can be written as L(D|S)*E, where | means 'or' and * indicates repetition. The application of a local refection model implies paths of type LD|S (the intensity of each being calculated separately then combined as in the Phong reflection model) and the addition of a hidden surface removal algorithm  implies simulation of types LD|SE. Thus local refection models  only simulate strings of length unity (between L and E) and viewing a point in shadow implies a string which is at least of length 2.

indnam
This is a ray traced image of the scene with the main light turned off emphasising that ray tracing omits all light paths except LDE and LDS*E. In this scene, which is not untypical of interiors, most of the global interaction is diffuse-diffuse (i.e. indirect lighting from the upward facing lights on the wall) and ray tracing is thus wholly inappropriate.

indamb
The previous scene with an ambient lift. The value of the ambient component is the same as that used to render the scene with the main light on (see rtfull), and is supposed to be a substitute for the illumination which would be present if diffuse-diffuse interation had been considered.

indrd
A radiosity rendered image with the main light turned off. The rest of the room is now visible since Radiosity methods correctly account for diffuse intereflections.

rad_ind
Radiance image with the main light turned off.


















STUDIES 9-13  RAY TRACING

STUDY 9 BASIC RAY TRACING

rtfull   
Basic/Whitted/Recursive/eye/Forward ray tracing of the standard scene. The differences between this and the Phong version are obvious. A few points are worth noting. The shadows are hard-edged which is the normal option in ray tracing. (At each intersection point a light ray is shot to the point source.) These look wrong compared to the options that can easily be implemented in shadow mapping (see  shmap). Another 'effect' that looks wrong is the blue glass vase by the door. Here the sharp refraction of the shadow makes the vase look as if it possesses a black band rather than refracting detail behind it.

There are around 10,000 polygons in this scene and using a normal/brute force intersection testing approach would make the scene 'unrenderable'. In this case a spatial partitioning (octree) approach was used.

An important comparison between ray tracing for specular interaction and environment mapping is that in ray tracing it is easy to assign specular reflectivity coefficients to as many objects in the scene as necessary. With environment mapping a separate map has to be created for each shiny object.


THE WHITTED IMAGE - BASIC RECURSIVE RAY TRACING

Raydemo  (note that this is an interactive program for PC's with Windows 95.  If you do not have this facility then the image whit_il can be used).
  
The image in the ray tracing demonstration is based on the  original image generated by Turner Whitted in 1981

We consider the way in which the ray tracing model works in the context of the 7 pixels shown highlighted. The scene itself consists of a thin walled or hollow sphere, that is almost perfectly transparent, together with a partially transparent white sphere, both of which are floating above the ubiquitous red and yellow chequerboard. Everywhere else in object space is a blue background. The object properties are summarised in the following table. Note that this model allows us to set ks to a different value from krg - the source of a contradiction in the model -reflected rays are treated differently depending on which component (local or global) is being considered.

very transparent  hollow sphere
kd  (local)            0.1  0.1  0.1  (low)
ks  (local)            0.8  0.8  0.8  (high)
krg                     0.1  0.1  0.1  (low)  
ktg                     0.9  0.9  0.9  (high)

Opaque (white) sphere
kd  (local)         0.2  0.2  0.2    (white)
ks  (local)          0.8  0.8  0.8   (white)
krg                    0.4  0.4  0.4   (white)
ktg                    0.0  0.0  0.0

Chequerboard
kd  (local)          1.0  0.0  0.0/1.0  1.0  0.0  (high  red or yellow)
ks   (local)         0.2  0.2  0.2
krg                   0
ktg                   0

Blue Background
kd  (local)        0.1  0.1  1.0  (high  blue)

Ambient light  0.3  0.3  0.3

light   0.7  0.7  0.7 


The rays associated with the pixels shown are:


Ray 1

This ray is along a direction where a specular highlight is seen on the highly transparent sphere. Because the ray is near the mirror direction of L, the contribution from the specular component in Ilocal(P) is high and the contributions from krgI(Pr) is low . For this object kd the local diffuse coefficient is low (it is multiplied by 1 - transparency value) and ks is high with respect to krg. However  note that the local contribution only dominates over a very small area of the surface of the object. Also note that, as we have already mentioned the highlight should not be spread. But if we left it as occupying a single  pixel it would not be visible. 

Ray 2

Almost the same as Ray 1 except that the specular highlight appears on the  inside wall of the hollow sphere. This particular ray demonstrates another accepted error in ray tracing. Effectively the ray from the light travels through the sphere without refracting (that is we simply compare L with the local value of N and ignore the fact that we are now inside a sphere). This means  that the specular highlight is in the wrong position but we simply accept this because we have no intuitive expectation of the correct position anyway. We simply accept it to be correct.

Ray 3

Ray 3 also hits the thin walled sphere. The local contribution at all hits with the hollow sphere are zero and the predominant contribution is the chequerboard. This is subject to slight distortion due to the refractive effect of the sphere walls. The red (or yellow) colour comes from the high  kd in Ilocal(P) where P is a point on the chequerboard. krg and ktg are zero for this surface. Note, however, that we have a mix of 2 chequerboards. One is as described and the other is the superimposed reflection on the outside surface of the sphere.

Ray 4

Again this hits the thin walled sphere, but this time in a direction where the distance travelled through the glass is significant (that is it only travels through the glass it does not hit the air inside) causing a high refractive effect and making the ray terminate in the blue background.

Ray 5

This ray hits the opaque sphere and returns a significant contribution from the local component due to a white kd (local). At the first hit the global reflected ray  hits  the chequerboard. Thus there is a mixture of:
white (from the spheres diffuse component)
red/yellow  (reflected from the chequerboard).

Ray 6

This ray hits the chequerboard initially  and the colour comes completely from the local component for that surface. However, the point is in shadow and this is discovered by the intersection of the ray L and the opaque sphere.

Ray  7

The situation with this ray is exactly the same as for ray 5 except that it is the thin walled sphere that intersects L . Thus the shadow area intensity is not reduced by as much as the previous case.  Again we do not consider the recursive effect that L would in fact experience and so the shadow is  in effect in the wrong place. 


















STUDY 10	 EFFECT OF RECURSIVE DEPTH

rtdth1-8

Perfect specular objects reflecting in each other persist as a popular ray tracing image to generate (although such scenes rarely occur in reality). This series was generated using the same scene with recursive depth varying for zero to 7. The teapot and the mirror are both perfect mirrors with no local reflection model parameters. For recursive depth of zero, rays terminate on the teapot and mirror surface and find no local component. They are (arbitrarily) rendered as grey. For recursive depth 1, both the teapot and the mirror find reflections but rays from the eye to the mirror to the teapot terminate at the teapot and produce a grey 'shadow' of the teapot in the mirror.  A particular point to note in this series is the way in which aliasing artefacts increase with recursive depth. (Do not read the next sentence with any alcohol in your blood.)  Zooming in on the centre of rtdth5 will confirm that 'the reflection of the light cable in the mirror in the teapot in the mirror' has completely broken up. This is, of course, due to the divergence of the initial bundle of rays as they continually encounter curved surfaces.

rtdaa 	 an anti-aliased version of rtdth8. Note that although the image quality is 	generally better the light cable still breaks up.




























STUDY 11	 WHY RAY TRACING DOES NOT WORK EXCEPT FOR SCENES THAT IT WORKS FOR

Not quite as tortuous as it sounds basic ray tracing only works well visually for perfect mirror like objects. Ray tracing combines a local refection model contribution with a global reflection and transmission component. The rationale here is that this approach can handle objects that are not perfectly reflecting or refracting. A ray traced image can be thus be viewed as a combination of a local component and a component from a perfect refractor or reflector.  This immediately raises the contradiction that a specular highlight is a spread reflection of the (necessarily) point light source but reflections of other objects in the same surface are perfect (not spread). Generally there is much confusion in the way in which local and global components are used as the next series of images attempts to demonstrate.

glas_m 	 Is a montage of the popular options available in 'naïve' ray tracing. The top left hand image is a pure local reflection model or Phong component which is kept constant throughout the array of images. As we go along the top row the global refection proportion increases to 100% and the Phong spread highlight eventually gives way to the actual image of the light source. The reason for keeping the Phong component constant rather than diminishing it as the global component takes over is that its contribution boosts the light level in the reflected image of the light source. For example consider the second image in the top row. 

Going down the first column, the transparency is increased towards 100%. Keeping the Phong component constant here now results in a contradiction in the bottom image which is that we have a refection of the light source and nothing else. However, in practice objects can never be perfectly refracting and we often see glass objects where we can only perceive the refection of the light source. In this image are also apparent geometric deficiencies in the refracted image. This is due to the polygonal representation of the object  and the way in which surface normals are derived. Ray tracing demands a surface normal at every hit point. This can never be the correct surface normal because the polygon mesh is already an approximation. Also apparent are bad aliasing artefacts along the silhouette edge of the object. Such defects are not so apparent in the perfect reflection image. In the refection image the refracted objects are much closer to the glass teapot.

glas11 to glas15
glas21 to glas24
glas31 to glas33
glas41 to glas42
glas51		 The individual images that make up the above montage.

gls_wrng	 This is an incorrect image created by setting both the reflection and transmission parameters to 1. The effect of this is for light to be 'created' whenever rays are spawned making it look as if the object is fluorescent.


STUDY  12	  DISTRIBUTED RAY TRACING ANIMATION

dof_im	An image rendered with a distributed ray tracer that illustrates depth of field variation

dof	An animation sequence that demonstrates the depth of field parameter in a distributed raytracer.

STUDY 13	 TWO-PASS RAY TRACING

caust1	 A study scene injected into a standard ray tracer

caust5	 
The same scene rendered using two-pass ray tracing. This accounts for LS*DE paths. The eye trace and the light trace 'meet' on a diffuse surface. The light pass  'sprinkles' the diffuse surfaces with light and the normal eye trace picks up these illumination changes on the diffuse surfaces. The LSDE paths involving refraction  or refection from a curved surface are called caustics. In this scene there are:

	two caustics from the red sphere - one directly from the light and one from the 	light reflected from the curved mirror.

	one (cusp) reflected caustic from the cylindrical mirror 

	secondary illumination from the planar mirror ( a non-caustic LSDE path)

This image illustrates some of the defects in ray tracing which are exacerbated  by two pass ray tracing. The combination of a local specular term with a global specular term is often implemented. This is inconsistent because we have a local term that spreads specular refection and a global term that does not. This results in the inconsistency of spread specular highlights of the light source in the spheres, but perfect specular refection of other objects. Another problem with ray tracing is the naïve combination  (or addition) of local and global components at an intersection point. No account is taken of correct energy and the refection of the table surface in the planar mirror is brighter than the table surface due to this.

caust2, caust3, caust4	
These images were produced by shooting an increasing number of light rays. As the number of rays in the light pass increases, these can eventually  be merged to form well defined LSDE paths in the image. The number of rays shot in the light pass was 200, 400, 800 and 1000 respectively. To constrain the light rays in the directions of interest some manual intervention was used in this program. Here a cone centred on the light source (equivalent to defining a spotlight, was set up for each object in such a way that all of the object was illuminated. 




STUDIES 14-18	RADIOSITY STUDIES

STUDY 14 	 BASIC CLASSICAL RADIOSITY (Hemi-cube evaluation of form factors, progressive refinement iteration, no subdivision).

hcr_w  
 wire frame of the scene used in this study. This scene has been carefully constructed to avoid problems that arise out of 'interpenetrating geometry'. Note that the table edge coincides with a subdivision line on the wall. When objects are treated as independently of each other, problems arise from light and shadow leakage. These are highlighted in STUDY 18.

hcube_fl	
This interesting image shows the state of a hemicube placed on the window after all other patches in the scene have been projected onto it. A colour identifies each patch in the scene (and every partial patch) that can be seen by this hemi-cube. The algorithm then simply summates all the hemicube element form factors associated with each patch.

hcube_m
hcube_t
hcube_b
hcube_l
hcube_r	Are the individual  hemicube face projections.

hcr_m   
image array of the following 12 images that are selected from the series
hcr1 to hcr8 illustrate the radiosity solution after 20, 70,120, 250, 500, 1000, 2000, 5000.

The individual images in the montage array are

hcr1a   to   hcr1d
hcr4a   to   hcr4d
hcr8a   to   hcr8d


From left to right the images are:

hcr*a	 (* represents a number from 1-8)
The radiosity solution as output from the iteration process. Each patch is allocated a constant radiosity.

hcr*b 	
The previous solution after it has been subject to a form of Gouraud interpolation.

hcr*c	
The same solution with  the addition of an ambient term. The addition of an ambient term gives an early 'full lit' solution. The value of the ambient term is calculated by considering the total unshot energy and distributing it evenly amongst all the patches in the scene. As the solution progresses the 'incorrect' ambient term reduces as the unshot energy is distributed correctly around the scene.

hcr*d  
The difference between *b and *c. This gives a visual indication of the energy that is to be added to account for the unshot energy. Going down the d column results in darker images as the ambient energy injected into the scene gets less and less

Going down the b column (no added ambient term) the image increase in brightness as more and more of the unshot energy is accounted for.

 Going down the c column (ambient  term included) the ambient energy decrease and the images get darker as they approach a solution. This because, although the added ambient term at each stage in the solution is calculated according to unshot energy, energy is being absorbed all the time because the patch reflectivities are less than unity. 

hcr8e	Final texture mapped solution

hcr8f	Radiosity with a ray tracing pass for the specular objects
























STUDY 15  COLOUR BLEEDING

Oft described in explanations of radiosity is the effect of colour bleeding. Because radiosity implements 'diffuse-diffuse' interaction, colour is 'carried' from surface to surface. The  apparentness of this effect in real life is debatable. In other words we may not perceive much colour bleeding - which could be measured instrumentally - because of the perceptual predominance of colour constancy. Certainly colour bleeding is fixed in photographs as the next image demonstrates.

museu1  to museu4	
A zoom series taken at midday in the bright light of Rio de Janeiro. The colour bleeding is immediately apparent in the photograph and persists even at a distance.

rdcb1  to rdcb3	
Radiosity images show colour bleeding from a coloured light source onto the walls and the colour bleeding that results from window light reflecting from a coloured object. These have been take from near convergence and the effect, although correct as far as the algorithm is concerned, somehow looks wrong.  This may be due in part to the different way in which our colour constancy works in a real environment compared with a viewing a photograph. The reflectivity of the cylinder was set at 0.85. It may also be a consequence of the fact that a solution is only computed at three wavelengths.


STUDY 16  - Classical Radiosity with subdivision 

 A detailed study of the progressive refinement solution that varies the parameters of the algorithm. These are options that trade time to completion against image quality and are in general:

the percentage of the total  initial energy that is shot before a solution is deemed to exist. The total energy unshot at any stage in the solution decreases because the patch reflectivities are less than unity by definition. 

parameters governing the degree of subdivision that takes place

parameters governing the accuracy of the form factor calculations (in a hemi-cube method the size of the hemi-cube)


These are grouped together in the following sequence of images under a general heading of 'quality' which has 6 settings.


QUALITY 1

rdq_wf  	initial mesh 

rdq1a
solution after shooting the light source patches. A light source patch deposits light source energy on all patches in the scene that it can see.

rdq1b 
the same solution as the previous image but with an 'ambient lift' added. rdq1c is the    state of the subdivision for this image. Subdivision proceeds continuously as the solution progresses. The radiosity gradient of the current state is examined and patches that exhibit a high gradient are sub-divided. 


rdq1d    
after the light source patches have been shot,  the complete list of patches is ordered according to the amount of energy that has fallen onto the patch and has yet to be shot back into the scene. The one with the highest unshot energy is selected and this is considered as the next shooting patch.  This patch is then put to the bottom of the list (because all its energy has been shot, and the solution continues. This image is the solution after this process has continued until 10 per cent of the total initial  energy has been distributed.

rdq1e  	as the previous image but with the ambient term added

rdq1f  		state of the sub-division for this image

rdq1g 		texture mapping can be added.

rdq1h  	as previous image with ambient term added

QUALITY 2

rdq2a - rdq2h

QUALITY 3
rdq3a- rdq3h

QUALITY 4
rdq4a - rdq4h

QUALITY 5
rdq5a - rdq5h

QUALITY6
rdq6a - rdq6h

Examination of the image and the mesh for the final solution illustrates a problem with the subdivision. This is that the mesh exhibits subdivided areas which appear not to be required. Look at 'streaks' near the door. Bear in mind that subdivisions occur after every patch is shot and areas that are subdivided because of this that may subsequently be illuminated by energy shot from other patches. See also the animation study mgen which illustrates this problem dynamically. 


STUDY 17	 ANIMATION OF PROGRESSIVE REFINEMENT BUILD UP

lwradp	 	

this animation shows the iteration progressing from a single light source (which is 140 triangular emitting patches) with no subdivision and no added ambient term. Consecutive frames show an equal increase (1%) in the amount of energy shot (rather than equal numbers of iterations). 


No of iterations required to achieve percentage light shot:

% light shot
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
no. iterations
0
7
14
19
24
29
33
38
43
47
51
55
58
61
64
68

% light shot
16
17
18
19
20
21
22
23
24
25
26
27
28
29
no. iterations
71
74
78
81
85
88
91
94
97
100
103
106
110
114

% light shot
30
31
32
33
34
35
36
37
38
39
40
41
42
no. iterations
118
121
125
128
132
136
140
144
151
156
161
167
173

% light shot
43
44
45
46
47
48
49
50
51
52
53
54
55
no. iterations
183
194
206
222
253
281
312
345
383
425
472
526
588

% light shot
56
57
58
59
60
61
62
63
64
65
66
67
no. iterations
661
750
861
997
1146
1317
1507
1715
1939
2231
2460
2756

% light shot
56
57
58
59
60
61
62
63
64
65
66
67
no. iterations
661
750
861
997
1146
1317
1507
1715
1939
2231
2460
2756

% light shot
68
69
70
71
72
73
74
75
76
77
78
no. iterations
3071
3400
3747
4109
4490
4883
5289
5499
5715
6162
6629

% light shot
79
80
81
82
83
84
85
86
87
88
no. iterations
7121
7645
8224
8862
9539
10242
10985
11772
12606
13536

% light shot
89
90
91
92
93
94
95
no. iterations
14590
15992
17670
19837
22797
26941
35637


Note that as the solution progresses the number of iterations required to shoot another 1% of the initial energy increases 'exponentially'. For example, going from 1% to 2% takes 7 iterations while going from 94 to 95% takes  around 10000 iterations. The solution is never complete - we can only approach it.


seq2_00  - seq2_95  the frames making up the animation


radwf		the wireframe used from the sequence




STUDY 18	   MESHING AND MESHING DEFECTS

rdleak  
 This image can be used to visualise shadow and light leakage. Zooming up to the left hand wall light will reveal substantial light leakage and also a small area of shadow leakage. Shadow leakage is apparent around the mirror frame. These problems occur because a basic approach only considers the relationship between patches in the scene completely ignoring the fact that their geometry can 'interact' with other objects. The wall patches that are producing light leakage have an area behind the light and those that produce shadow leakage have areas behind the mirror. In the examples hcr1a etc. the scene was carefully constructed to take into account the 'interpenetrating' geometry. For example, the wall behind the table is subdivided along a line that coincides with a table edge.

Adaptive  subdivision attempts to cope with this as the solution progresses. (However, as we pointed out above this in itself suffers from the problem that an area may be finely subdivided then this subdivision obviated by subsequent reception of energy from a previously unshot source)

rdam_wf 	
A  uniform subdivision of the standard scene - all non-triangular polygons are converted into triangular patches.

rdmm1,  rdmm2, rdmm3	
A uniform subdivision to create 3000 patches and the resulting low quality solution.

rdam1, rdam2, rdam3	
Using subdivision during the solution. In this case the subdivision was terminated when it resulted in 3000 patches - the same as before. The difference in quality is obvious between the two series

mgen  
This is an animation showing the build up of subdivision around one of the wall lights.  The animation proceeds as follows. Originally two large patches situated away from the wall provide general illumination of the object. This immediately causes subdivision around the light/wall boundary because the program detects a high difference between vertices belonging to the same patches. These patches have vertices both under the light and on the wall. However, this subdivision is not fine enough and as we start to shoot energy from the light source itself light leakage begins to occur.  Light source patches continue to shoot energy in the order in which the model was originally created and we spiral up the sphere, shooting energy onto its inside and causing more and more light leakage. Eventually the light emerges onto the wall and brights up the appropriate patches. As the fan of light rotates above the light more and more inappropriate subdivision occurs. This is because the subdivision is based on the current intensity gradients which move on as further patches are shot. Note in the final frame this results in a large degree of subdivision in an area of high light saturation. These redundant patches slow the solution down more and more and we are inadvertently making things worse.

This problem leads to an alternative strategy which is to:

1) 	Limit the initiation of subdivision by only initiating after every n patches instead of after every patch that is shot.
2) 	Limit the initiation of sub-division by waiting until the illumination is representative of the expected final distribution.
3) 	Aid the initial mesh generation by taking account of interpenetrating geometry. This means adding edges to objects that coincide with boundaries of other objects. This solution is applied before the radiosity solution begins.

rdintm and rdintp     
	the  result of meshing the area around a wall light after considering the 	interpenetrating geometry. Now the wall patch boundaries coincide with the 	light patch boundaries. The result of this mesh completely eliminates the leakage 	shown in rdleak  which is based on the mesh shown in rdlkm


rdfull	this show the scene rendered using a combination of radiosity and raytracing. The initial stage involved computing a high quality radiosity solution (note that shadow and light leakage around the lights has been avoided by considering the interpenetrating geometry). This solution is then combined with a raytraced solution, which accounts for the mirrored surfaces in the scene.


STUDY 19	RADIANCE

scene-low	
low qualty solution (approximately 2 mins execution time SUN SPARC 10).  All diffuse-diffuse interaction is turned off).

scene_med 
	medium quality solution (approx 1hr execution time).

scene_high_1
	high quality solution with same material properties (appro 20hrs execution time).
scene_high_2
	high quality with textures added and different material types.
scene_high_3		
This shows a LSDE path that has been rendered by RADIANCE. The extra light on the right hand wall comes from light reflected from the mirror. This bright patch would not appear with any of the other renderers used in this series.
                    


STUDY 20	 OBVIOUS FINAL DIFFERENCES 
COLOUR

It is instructive to compare, say, scanln (scan line rendered based on Phong illumination model), raytr (Whitted ray traced), radios (hybrid radiosity-raytraced) and radnce (RADIANCE).  The scene database used in all cases is identical and the rendering was evaluated at three (RGB) spectral points. Note that the colours in the Phong and the ray traced image are the same as we would expect; but that there are colour shifts in the radiosity and the RADIANCE images. There are a number of reasons for this.

1) 	In the radiosity image we are simulating diffuse-diffuse interaction which 'carries' colour from surface to surface - the characteristic colour bleeding effect (see STUDY 15).  The extent to which this happens depends upon spatial relationship of the patches, their diffuse reflectivity and the original colours of the patches which are interacting. It is easy to get 'too much' colour bleeding by having high reflectivities (see, for example, rdcb2) and this is yet another aspect of a radiosity image that requires 'user tuning'. In Phong and ray tracing the diffuse reflectivity coefficients exclusively determine the colour of diffuse surfaces. 

2) 	In the RADIANCE image extra paths are accounted for (LS*DE) paths which will also effect the colour. 

3)  Different rendering methods have different facilities which can effect the final colour. For example, with RADIANCE, the solution produced is such that each pixel has three values associated with it which give the amount of light energy entering the camera from that particular direction at each of the Red, Green and Blue wavelengths. A final process is then carried out which converts this "power" image into a standard image based on an "exposure" parameter (equivalent to the exposure on a camera). This process is ignored in radiosity, phong and raytraced images.


IMAGE DEFINITION

Another notable difference is that the RADIANCE image looks fuzzy.  This is probably a combination of factor 3) above and the fact that Radiance is combining information from non-uniform sampling of the scene (due to the anti-aliasing method used).

LIGHT LEVELS 

The approach to the setting of light levels when creating a scene varies based on the method used. This ranges from arbitrary values for the ambient, point light, and spot lights generaly used in scanline/raytraced scenes, to physical units (relating to light power output) used in RADIANCE. Attempting to choose values which will give similar results for each method is usually a matter of trial and error.


STUDY 21 NON-PHOTO REALISTIC RENDERING

The following images are simple  examples of so-called non-photorealistic rendering. They were produced by post processing the output from a conventional renderer together with Z-buffer information

corra	is a radiosity simulation of a corridor scene.
corrb	is an image produced by passing the original image through an edge extraction filter.
corrc 	image showing brush strokes/
corrd	image combining edge image (corrb) and brush stroke image (corrc).

sunara	(same as above for a different scene).
sunarb	
sunarc	
sunard	

wida	is a conventional rendered representation, using stylised colours and shadows
widb	is a simple allusion to cross-hatching. The intensity of the final rendered image 	is coarsely quantized and used to modulate the horizontal lines.
widc	This 'brush stroke' image was produced by point sampling the rendered image  	to obtain basic brush stroke attributes. These are then sorted in depth using the 	Z-buffer information at each sample point before being drawn. 
widd	line drawing combined with original image (wida+widb)

urn	NPR urn
tea	NPR teapot

	


